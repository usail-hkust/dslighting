"""
Base Workflow Factory - Base class for all Workflow Factory

Provides standard MLE task loading functionality, users don't need to reimplement
"""

import logging
from pathlib import Path
from typing import Dict, Any, Optional
from abc import ABC, abstractmethod

logger = logging.getLogger(__name__)


class BaseWorkflowFactory(ABC):
    """
    Base class for Workflow Factory

    Provides:
    1. Standard LLM/Sandbox/Workspace service creation
    2. Standard MLE task loading (from registry)
    3. run_with_task_id() convenience method

    Users only need to:
    1. Inherit from BaseWorkflowFactory
    2. Implement create_agent() method
    3. Define their own workflow class
    """

    def __init__(
        self,
        model: str = "gpt-4o",
        api_key: str = None,
        api_base: str = None,
        provider: str = None,
        temperature: float = None,
        timeout: int = 300,
        keep_workspace: bool = False,
        **agent_init_kwargs
    ):
        """
        Initialize factory

        Args:
            model: LLM model name
            api_key: API key (optional, read from env var if not provided)
            api_base: API base URL (optional, read from env var if not provided)
            provider: LLM provider (optional)
            temperature: Temperature parameter (optional, read from env var if not provided)
            timeout: Sandbox timeout
            keep_workspace: Whether to keep workspace
            **agent_init_kwargs: Additional parameters, will be passed to create_agent()

        Note:
            Use DSLighting's ConfigBuilder to automatically read config from environment variables:
            - API_KEY, API_BASE, LLM_MODEL
            - LLM_MODEL_CONFIGS (multi-model config)

        Example:
            >>> factory = MyWorkflowFactory(
            ...     model="gpt-4o",
            ...     max_iterations=3,  # Passed to create_agent()
            ...     use_data_insights=True
            ... )
        """
        self.model = model
        self.timeout = timeout
        self.keep_workspace = keep_workspace
        self._agent_init_kwargs = agent_init_kwargs

        # Use DSLighting's ConfigBuilder to automatically read config from environment variables
        from dslighting.core.config_builder import ConfigBuilder
        config_builder = ConfigBuilder()
        config = config_builder.build_config(
            model=model,
            api_key=api_key,
            api_base=api_base,
            provider=provider,
            temperature=temperature,
        )

        # Extract LLM config from configuration
        llm_config = config.llm

        # Create services (infrastructure ready, users don't need to care)
        from dslighting.services import LLMService, SandboxService, WorkspaceService

        self.llm_service = LLMService(config=llm_config)
        self.workspace_service = WorkspaceService(
            run_name=f"{self._get_workflow_name()}_{model.replace('/', '_')}"
        )
        self.sandbox_service = SandboxService(
            workspace=self.workspace_service,
            timeout=timeout
        )

        logger.debug(f"{self.__class__.__name__} initialized")
        logger.debug(f"  - Model: {model}")
        logger.debug(f"  - Timeout: {timeout}s")
        logger.debug(f"  - Keep workspace: {keep_workspace}")

    def _get_workflow_name(self) -> str:
        """
        获取 workflow 名称（用于日志和 workspace 命名）

        子类可以重写此方法以提供自定义名称
        """
        return self.__class__.__name__.replace("Factory", "").lower()

    @abstractmethod
    def create_agent(self, **kwargs) -> Any:
        """
        创建 Agent 实例（子类必须实现）

        Args:
            **kwargs: Agent 配置参数

        Returns:
            Agent 实例
        """
        raise NotImplementedError("Subclasses must implement create_agent()")

    def cleanup(self):
        """清理工作空间"""
        if not self.keep_workspace:
            self.workspace_service.cleanup()
            logger.info(f"✓ 工作空间已清理")

    def run(
        self,
        data=None,
        task_id: Optional[str] = None,
        data_dir: Optional[Path] = None,
        **kwargs
    ):
        """
        运行 workflow - 统一的入口（推荐使用）

        这是同步方法，用户无需关心 async/await

        支持多种调用方式：

        1. **使用 LoadedData 对象**（最简单）:
           >>> data = dslighting.load_data("/path/to/data")
           >>> result = factory.run(data)  # ← 不需要 await

        2. **使用 task_id**:
           >>> result = factory.run(task_id="bike-sharing-demand")

        3. **使用 task_id + data_dir**:
           >>> result = factory.run(
           ...     task_id="bike-sharing-demand",
           ...     data_dir="/path/to/data"
           ... )

        4. **使用 dataset 字典**（从 datasets.load_xxx() 返回的）:
           >>> dataset = dslighting.datasets.load_bike_sharing_demand()
           >>> result = factory.run(dataset)

        Args:
            data: 可选，可以是：
                - LoadedData 对象（从 dslighting.load_data() 返回）
                - dataset 字典（从 dslighting.datasets.load_xxx() 返回）
                - 如果提供，将从中提取 task_id 和 data_dir
            task_id: 任务 ID（例如 "bike-sharing-demand"）
                - 如果不提供且 data 也不提供，需要单独指定 data_dir
            data_dir: 数据目录路径
            **kwargs: 传递给 create_agent() 的参数

        Returns:
            执行结果

        Example:
            >>> factory = MyWorkflowFactory(model="gpt-4o")

            >>> # 方式 1: 使用 LoadedData（不需要 await）
            >>> data = dslighting.load_data("/path/to/data")
            >>> result = factory.run(data)

            >>> # 方式 2: 使用 task_id（不需要 await）
            >>> result = factory.run(task_id="bike-sharing-demand")

            >>> # 方式 3: 使用 dataset 字典（不需要 await）
            >>> dataset = dslighting.datasets.load_bike_sharing_demand()
            >>> result = factory.run(dataset)
        """
        import asyncio
        return asyncio.run(self._run_async(data=data, task_id=task_id, data_dir=data_dir, **kwargs))

    async def _run_async(
        self,
        data=None,
        task_id: Optional[str] = None,
        data_dir: Optional[Path] = None,
        **kwargs
    ):
        """
        运行 workflow - 统一的入口（推荐使用）

        支持多种调用方式：

        1. **使用 LoadedData 对象**（最简单）:
           >>> data = dslighting.load_data("/path/to/data")
           >>> await factory.run(data)

        2. **使用 task_id**:
           >>> await factory.run(task_id="bike-sharing-demand")

        3. **使用 task_id + data_dir**:
           >>> await factory.run(
           ...     task_id="bike-sharing-demand",
           ...     data_dir="/path/to/data"
           ... )

        4. **使用 dataset 字典**（从 datasets.load_xxx() 返回的）:
           >>> dataset = dslighting.datasets.load_bike_sharing_demand()
           >>> await factory.run(dataset)

        Args:
            data: 可选，可以是：
                - LoadedData 对象（从 dslighting.load_data() 返回）
                - dataset 字典（从 dslighting.datasets.load_xxx() 返回）
                - 如果提供，将从中提取 task_id 和 data_dir
            task_id: 任务 ID（例如 "bike-sharing-demand"）
                - 如果不提供且 data 也不提供，需要单独指定 data_dir
            data_dir: 数据目录路径
            **kwargs: 传递给 create_agent() 的参数

        Example:
            >>> factory = MyWorkflowFactory(model="gpt-4o")

            >>> # 方式 1: 使用 LoadedData
            >>> data = dslighting.load_data("/path/to/data")
            >>> await factory.run(data)

            >>> # 方式 2: 使用 task_id
            >>> await factory.run(task_id="bike-sharing-demand")

            >>> # 方式 3: 使用 dataset 字典
            >>> dataset = dslighting.datasets.load_bike_sharing_demand()
            >>> await factory.run(dataset)
        """
        # 情况 1: 提供了 data 参数
        if data is not None:
            # 检查 data 的类型
            if hasattr(data, 'task_id') and hasattr(data, 'data_dir'):
                # LoadedData 对象
                logger.info(f"✓ 检测到 LoadedData 对象")
                task_id = data.task_id
                data_dir = data.data_dir
            elif isinstance(data, dict) and 'data_dir' in data:
                # dataset 字典（从 dslighting.datasets.load_xxx() 返回）
                logger.info(f"✓ 检测到 dataset 字典")
                data_dir = Path(data['data_dir'])
                task_id = task_id or data.get('task_id')
            else:
                raise ValueError(
                    f"不支持的 data 类型: {type(data)}\n"
                    f"期望: LoadedData 对象或 dataset 字典"
                )

        # 情况 2: 只提供了 task_id
        elif task_id is not None and data_dir is None:
            # 从 registry 自动查找 data_dir
            logger.info(f"✓ 只提供 task_id，将从 registry 自动查找 data_dir")
            # 调用 run_with_task_id，让它的内部逻辑处理 data_dir 查找
            return await self.run_with_task_id(task_id=task_id, **kwargs)

        # 情况 3: 必须提供 task_id 和 data_dir
        if task_id is None:
            raise ValueError("必须提供 task_id 参数（或者提供包含 task_id 的 data 对象）")
        if data_dir is None:
            raise ValueError("必须提供 data_dir 参数（或者提供包含 data_dir 的 data 对象）")

        # 调用 run_with_task_id
        return await self.run_with_task_id(
            task_id=task_id,
            data_dir=Path(data_dir) if not isinstance(data_dir, Path) else data_dir,
            **kwargs
        )

    async def run_with_task_id(
        self,
        task_id: str,
        data_dir: Optional[Path] = None,
        task_loader: Optional[Any] = None,
        output_path: Optional[Path] = None,
        **agent_kwargs
    ) -> None:
        """
        使用 task_id 运行 workflow（类似 DSLighting 的 run_agent）

        这是推荐的用法 - 自动从 registry 加载标准 MLE 格式配置

        Args:
            task_id: 任务 ID（例如 "bike-sharing-demand"）
            data_dir: 可选的数据目录路径。如果不提供，将从 registry 自动查找
            task_loader: 可选的任务加载器。如果不提供，使用 MLETaskLoader
            output_path: 可选的输出文件路径。如果不提供，使用 task_loader 返回的默认路径
            **agent_kwargs: 传递给 create_agent() 的参数（例如 max_iterations）

        Example:
            >>> factory = MyWorkflowFactory(model="gpt-4o")
            >>> await factory.run_with_task_id("bike-sharing-demand", max_iterations=3)

            >>> # 指定输出文件名
            >>> await factory.run_with_task_id("bike-sharing-demand", output_path="my_submission.csv")
        """
        logger.info(f"=" * 80)
        logger.info(f"运行 {self.__class__.__name__} with task_id")
        logger.info(f"=" * 80)
        logger.info(f"  Task ID: {task_id}")
        logger.info(f"  Agent Config: {agent_kwargs}")
        logger.info(f"=" * 80)

        # ✅ 使用 Task Loader 加载任务（从 tasks 层）
        if task_loader is None:
            from dslighting.tasks import MLETaskLoader
            task_loader = MLETaskLoader()

        # ✅ 对于 MLE 格式，只分析 public 数据（避免泄露 private/test_answer.csv）
        public_dir = data_dir / "prepared" / "public"

        if not public_dir.exists():
            logger.error(f"❌ Public data directory not found: {public_dir}")
            logger.error(f"Expected structure: {data_dir}/prepared/public/train.csv")
            raise FileNotFoundError(
                f"Public data directory not found: {public_dir}\n"
                f"Expected structure: {data_dir}/prepared/public/train.csv"
            )

        logger.info(f"✓ 使用 public 数据目录（避免泄露答案）: {public_dir}")

        # 加载标准 MLE 格式任务配置（传递 public_dir 而不是 data_dir）
        description, io_instructions, _, default_output_path = task_loader.load_task(
            task_id=task_id,
            data_dir=public_dir  # ✅ 只分析 public 目录
        )

        # ✅ 如果用户提供了 output_path，使用用户的；否则使用默认的
        output_path = output_path or default_output_path

        # ✅ 验证加载结果
        logger.info(f"✓ 任务加载完成:")
        logger.info(f"  - Description 长度: {len(description)} 字符")
        logger.info(f"  - I/O Instructions 长度: {len(io_instructions)} 字符")
        logger.info(f"  - Public 目录: {public_dir}")
        logger.info(f"  - 输出路径: {output_path}")

        # ✅ 自动处理数据链接（基础设施层，用户无需关心）
        # 将 public_dir 的内容链接到 sandbox 根目录
        logger.info(f"✓ 自动链接 public 数据到 sandbox...")
        logger.info(f"  源目录: {public_dir}")
        self.workspace_service.link_data_to_workspace(public_dir)
        logger.info(f"  ✓ Sandbox 已准备就绪")

        # ✅ 检查 io_instructions 是否完整（应该包含 "CRITICAL I/O REQUIREMENTS"）
        if len(io_instructions) < 100 or "CRITICAL I/O" not in io_instructions:
            logger.warning(f"⚠️ I/O Instructions 可能不完整！长度: {len(io_instructions)}")
            logger.warning(f"  io_instructions 前200字符: {io_instructions[:200]}")
            logger.warning(f"  这可能导致模型无法正确理解文件路径要求！")
            logger.warning(f"  尝试重新生成完整的 I/O instructions...")

            # ✅ 尝试重新生成完整的 I/O instructions
            try:
                from dsat.services.data_analyzer import DataAnalyzer
                analyzer = DataAnalyzer()
                io_instructions = analyzer.generate_io_instructions(
                    output_path.name,
                    optimization_context=False
                )
                logger.info(f"✓ 重新生成 I/O instructions 成功！长度: {len(io_instructions)}")
            except Exception as e:
                logger.error(f"重新生成失败: {e}")
                # 最后的回退：使用硬编码的格式
                io_instructions = f"""
--- CRITICAL I/O REQUIREMENTS ---

You MUST follow these file system rules precisely. Failure to do so will cause a fatal error.

1. **INPUT DATA:**
   - All input files are located in the **current working directory** (./).
   - Example: Use `pd.read_csv('train.csv')`.

2. **OUTPUT FILE:**
   - You MUST save your final submission file to the **current working directory** (./).
   - The required output filename is: `{output_path.name}`
   - **Correct Example:** `submission_df.to_csv('{output_path.name}', index=False)`

**IMPORTANT:** These path requirements are non-negotiable and must be followed exactly.
"""

        # 创建 agent（合并 __init__ 时保存的参数和运行时参数）
        all_agent_kwargs = {**self._agent_init_kwargs, **agent_kwargs}
        agent = self.create_agent(**all_agent_kwargs)

        # ✅ 记录开始时间（用于计算 duration）
        import time
        start_time = time.time()

        # 运行 workflow（传递 public_dir 给 workflow）
        await agent.solve(
            description=description,
            io_instructions=io_instructions,  # ✅ 包含输出文件名要求
            data_dir=public_dir  # ✅ 只传递 public 目录给 workflow
        )

        # ✅ 计算执行时间
        duration = time.time() - start_time

        # ✅ 自动评分（基础设施，用户无需关心）
        logger.info(f"\n{'='*80}")
        logger.info(f"自动评分中...")
        logger.info(f"{'='*80}")

        score = None
        try:
            # 获取提交文件路径
            submission_file = self.workspace_service.get_path("sandbox_workdir") / output_path.name

            if submission_file.exists():
                logger.info(f"✓ 提交文件: {submission_file}")

                # ✅ 通用评分逻辑：尝试多种方式加载 benchmark
                benchmark = None
                benchmark_loaded = False

                # 方式 1: 检查 task_loader 是否有 load_benchmark 方法
                if hasattr(task_loader, 'load_benchmark'):
                    try:
                        logger.info(f"尝试使用 task_loader.load_benchmark()...")
                        benchmark = task_loader.load_benchmark(
                            task_id=task_id,
                            data_dir=data_dir
                        )
                        if benchmark:
                            benchmark_loaded = True
                            logger.info(f"✓ 通过 task_loader 加载 benchmark")
                    except Exception as e:
                        logger.warning(f"task_loader.load_benchmark() 失败: {e}")

                # Fallback 2: Try loading directly from benchmarks/mlebench
                if not benchmark_loaded:
                    try:
                        logger.info(f"Attempting to load directly from benchmark/mlebench...")
                        from pathlib import Path as LibPath
                        benchmarks_dir = LibPath(__file__).parent.parent.parent / "benchmark" / "mlebench"
                        from dslighting.benchmark.mlebench.registry import Registry

                        registry = Registry(benchmarks_dir)
                        competition = registry.get_competition(task_id)

                        if competition:
                            # Create simple benchmark wrapper
                            class DirectBenchmark:
                                def __init__(self, comp):
                                    self.competition = comp

                                async def grade(self, submission_path: str):
                                    from dslighting.benchmark.mlebench.grade import grade_csv
                                    report = grade_csv(LibPath(submission_path), self.competition)
                                    return {
                                        'score': report.score,
                                        'valid_submission': report.valid_submission
                                    }

                            benchmark = DirectBenchmark(competition)
                            benchmark_loaded = True
                            logger.info(f"Loaded benchmark directly from MLE-Bench")
                    except Exception as e:
                        logger.warning(f"Failed to load MLE-Bench directly: {e}")

                # Fallback 3: Use universal grading (check file format)
                if not benchmark_loaded:
                    logger.info(f"Using universal grading logic...")
                    try:
                        import pandas as pd
                        # Check if file can be read normally
                        df = pd.read_csv(submission_file)
                        logger.info(f"Valid submission file: {len(df)} rows")

                        # Universal grading: file exists and is readable = success
                        # (Cannot calculate real score without ground truth)
                        score = 0.0
                        logger.info(f"Universal grading: file valid but cannot calculate real score (requires ground truth)")
                        logger.info(f"Tip: Implement task_loader.load_benchmark() method to get real score")
                    except Exception as e:
                        logger.warning(f"Universal grading failed: {e}")

                # If benchmark was successfully loaded, use it for grading
                if benchmark_loaded and benchmark and hasattr(benchmark, 'grade'):
                    try:
                        # Call benchmark.grade() for grading
                        grade_result = await benchmark.grade(
                            submission_path=str(submission_file)
                        )

                        # Extract score (grade_result may be dict or object)
                        if isinstance(grade_result, dict):
                            score = grade_result.get('score', grade_result.get('metric', 0.0))
                        else:
                            score = float(grade_result) if grade_result is not None else 0.0

                        logger.info(f"Auto-grading completed | Score: {score}")
                    except Exception as e:
                        logger.warning(f"Benchmark grading failed: {e}")
                        logger.warning(f"   Will fall back to universal grading")
                        score = 0.0
            else:
                logger.warning(f"Submission file not found: {submission_file}")
                logger.warning(f"   Workflow execution failed, cannot grade")

        except Exception as e:
            logger.warning(f"Auto-grading failed: {e}")
            logger.warning(f"   Please check submission file format and benchmark configuration")

        logger.info(f"{'='*80}\n")

        # ✅ 构建结果对象
        from types import SimpleNamespace
        result = SimpleNamespace()

        # 判断成功与否：提交文件存在且有评分
        result.score = score if score is not None else 0.0
        result.success = score is not None
        result.error = None if score is not None else "Grading failed or submission not found"

        # 获取成本（从 LLM service）
        result.cost = self.llm_service.get_total_cost() if hasattr(self.llm_service, 'get_total_cost') else 0.0
        result.duration = duration

        logger.info(f"=" * 80)
        logger.info(f"✓ Workflow 完成")
        logger.info(f"  - Success: {result.success}")
        logger.info(f"  - Score: {result.score}")
        logger.info(f"  - Cost: ${result.cost:.4f}")
        logger.info(f"  - Duration: {result.duration:.2f}s")
        logger.info(f"=" * 80)

        # ✅ 返回结果对象
        return result
