#!/usr/bin/env python3
"""
æ‰¹é‡è½¬æ¢ DABench ä»»åŠ¡åˆ° MLE-Bench æ ¼å¼

ä½¿ç”¨æ–¹æ³•:
    python convert_dabench_to_mlebench.py --task-ids 0 5 6
    python convert_dabench_to_mlebench.py --all  # è½¬æ¢æ‰€æœ‰ä»»åŠ¡
"""
import json
import re
from pathlib import Path
import argparse
from typing import Dict, List, Any


# è·¯å¾„é…ç½®
DABENCH_DIR = Path('/path/to/DABench')
MLEBENCH_COMPETITIONS_DIR = Path('/path/to/data_science_agent_toolkit/mlebench/competitions')
DSFLOW_DATA_DIR = Path('/path/to/mlebench-data')


def load_dabench_data():
    """åŠ è½½ DABench çš„é—®é¢˜å’Œç­”æ¡ˆ"""
    questions_file = DABENCH_DIR / 'da-dev-questions.jsonl'
    labels_file = DABENCH_DIR / 'da-dev-labels.jsonl'

    questions = {}
    with open(questions_file, 'r') as f:
        for line in f:
            q = json.loads(line)
            questions[q['id']] = q

    labels = {}
    with open(labels_file, 'r') as f:
        for line in f:
            l = json.loads(line)
            labels[l['id']] = l

    return questions, labels


def format_answer(common_answers: List[List[str]]) -> str:
    """å°† common_answers æ ¼å¼åŒ–ä¸ºç­”æ¡ˆå­—ç¬¦ä¸²"""
    parts = []
    for key, value in common_answers:
        parts.append(f"@{key}[{value}]")
    return " ".join(parts)


def generate_sample_answer(common_answers: List[List[str]]) -> str:
    """ç”Ÿæˆç¤ºä¾‹ç­”æ¡ˆï¼ˆå…¨éƒ¨å¡«0æˆ–é»˜è®¤å€¼ï¼‰"""
    parts = []
    for key, value in common_answers:
        # å°è¯•åˆ¤æ–­å€¼çš„ç±»å‹
        try:
            float(value)
            default_value = "0.00"
        except:
            default_value = "unknown"
        parts.append(f"@{key}[{default_value}]")
    return " ".join(parts)


def create_competition_id(task_id: int, question: str) -> str:
    """ç”Ÿæˆæ¯”èµ› ID"""
    # ä»é—®é¢˜ä¸­æå–å…³é”®è¯
    words = re.findall(r'\b[a-z]+\b', question.lower())
    keywords = [w for w in words if len(w) > 3 and w not in ['calculate', 'create', 'perform', 'apply', 'check']]
    suffix = '-'.join(keywords[:3]) if keywords else 'task'
    return f"dabench-{task_id}-{suffix}"


def create_config_yaml(comp_id: str, task_name: str) -> str:
    """ç”Ÿæˆ config.yaml å†…å®¹"""
    return f"""id: {comp_id}
name: "DABench Task - {task_name}"
competition_type: code
awards_medals: false
prizes: null
description: mlebench/competitions/{comp_id}/description.md

dataset:
  answers: {comp_id}/prepared/private/answer.csv
  sample_submission: {comp_id}/prepared/public/sample_submission.csv

grader:
  name: exact_match
  grade_fn: mlebench.competitions.{comp_id}.grade:grade

preparer: mlebench.competitions.{comp_id}.prepare:prepare
"""


def create_description_md(question_data: Dict) -> str:
    """ç”Ÿæˆ description.md å†…å®¹"""
    concepts_str = ", ".join(question_data['concepts'])

    return f"""# DABench Task {question_data['id']} - {question_data['question']}

## Task Description

{question_data['question']}

## Concepts

{concepts_str}

## Data Description

Dataset file: `{question_data['file_name']}`

The data is available in the `train.csv` file in the public directory.

## Constraints

{question_data['constraints']}

## Files

- `train.csv` - The training data
- `sample_submission.csv` - A sample submission file in the correct format

## Submission Format

{question_data['format']}

## Evaluation

Submissions are evaluated on **exact match** with the ground truth answer (with tolerance of 0.01 for floating-point values).

## Goal

Provide the correct answer in the specified format.

## Difficulty Level

{question_data['level'].capitalize()}
"""


def create_grade_py(common_answers: List[List[str]]) -> str:
    """ç”Ÿæˆ grade.py å†…å®¹"""
    # æå–æ‰€æœ‰çš„ key åç§°
    keys = [key for key, _ in common_answers]

    return f"""import pandas as pd
import re


def grade(submission: pd.DataFrame, answers: pd.DataFrame) -> float:
    \"\"\"
    Grade the DABench submission.

    Args:
        submission: DataFrame with columns ['id', 'answer']
        answers: DataFrame with columns ['id', 'answer']

    Returns:
        float: 1.0 if exact match, 0.0 otherwise
    \"\"\"
    try:
        # Both should have exactly one row
        if len(submission) != 1 or len(answers) != 1:
            return 0.0

        # Get the submission and answer strings
        submission_str = str(submission.iloc[0]['answer']).strip()
        answer_str = str(answers.iloc[0]['answer']).strip()

        # Parse all key-value pairs
        # Expected keys: {keys}
        pattern = r'@(\\w+)\\[([^\\]]+)\\]'

        submission_dict = dict(re.findall(pattern, submission_str))
        answer_dict = dict(re.findall(pattern, answer_str))

        if not submission_dict or not answer_dict:
            print(f"Failed to parse: submission='{{submission_str}}', answer='{{answer_str}}'")
            return 0.0

        # Check if all keys match
        if set(submission_dict.keys()) != set(answer_dict.keys()):
            print(f"Key mismatch: submission has {{set(submission_dict.keys())}}, answer has {{set(answer_dict.keys())}}")
            return 0.0

        # Compare values
        all_match = True
        for key in answer_dict:
            submission_value = submission_dict[key]
            answer_value = answer_dict[key]

            # Try to compare as numbers
            try:
                sub_float = float(submission_value)
                ans_float = float(answer_value)
                if abs(sub_float - ans_float) >= 0.01:
                    print(f"Value mismatch for {{key}}: submission={{sub_float}}, answer={{ans_float}}")
                    all_match = False
                    break
            except ValueError:
                # Compare as strings (case-insensitive)
                if submission_value.lower() != answer_value.lower():
                    print(f"Value mismatch for {{key}}: submission='{{submission_value}}', answer='{{answer_value}}'")
                    all_match = False
                    break

        return 1.0 if all_match else 0.0

    except Exception as e:
        print(f"Error in grading: {{e}}")
        return 0.0
"""


def create_prepare_py(task_id: int, file_name: str, answer_str: str) -> str:
    """ç”Ÿæˆ prepare.py å†…å®¹"""
    # Escape single quotes in answer_str and use double quotes for the string
    answer_str_escaped = answer_str.replace('\\', '\\\\').replace('"', '\\"')

    return f"""from pathlib import Path
import pandas as pd


def prepare(raw: Path, public: Path, private: Path):
    \"\"\"
    Prepare the DABench task {task_id} dataset.

    Args:
        raw: Path to raw data directory (should contain {file_name})
        public: Path to public directory (for participants)
        private: Path to private directory (for grading)
    \"\"\"
    # Load the data
    data_file = raw / "{file_name}"
    if not data_file.exists():
        raise FileNotFoundError(f"Data file not found: {{data_file}}")

    df = pd.read_csv(data_file)

    # Save the full dataset to public directory
    train_file = public / "train.csv"
    df.to_csv(train_file, index=False)
    print(f"Saved training data to {{train_file}} ({{len(df)}} rows)")

    # Create sample submission file
    sample_submission = pd.DataFrame({{
        'id': [{task_id}],
        'answer': ['@placeholder[0.00]']  # Placeholder answer
    }})
    sample_submission_file = public / "sample_submission.csv"
    sample_submission.to_csv(sample_submission_file, index=False)
    print(f"Created sample submission: {{sample_submission_file}}")

    # Create answer file (ground truth)
    answer = pd.DataFrame({{
        'id': [{task_id}],
        'answer': ["{answer_str_escaped}"]
    }})
    answer_file = private / "answer.csv"
    answer.to_csv(answer_file, index=False)
    print(f"Created answer file: {{answer_file}}")

    # Verify
    assert train_file.exists(), "Training file not created"
    assert sample_submission_file.exists(), "Sample submission not created"
    assert answer_file.exists(), "Answer file not created"

    print(f"âœ“ DABench task {task_id} prepared successfully")
"""


def create_leaderboard_csv() -> str:
    """ç”Ÿæˆ leaderboard.csv å†…å®¹"""
    return """scoreNullable,teamId,hasTeamName,submissionDate,score,hasScore
1.0000,1,True,2024-01-01 00:00:00,1.0000,True
0.0000,2,True,2024-01-01 00:00:00,0.0000,True
"""


def create_checksums_yaml(comp_id: str) -> str:
    """ç”Ÿæˆ checksums.yaml å†…å®¹"""
    return f"""# Checksums for {comp_id} dataset
zip: ""
"""


def create_dataset_prepare_py(comp_id: str) -> str:
    """ç”Ÿæˆæ•°æ®é›†ç›®å½•çš„ prepare.py è„šæœ¬"""
    return f"""#!/usr/bin/env python3
\"\"\"
æ•°æ®å‡†å¤‡è„šæœ¬ - {comp_id}

ä½¿ç”¨ï¼š
    cd /path/to/mlebench-data/{comp_id}
    python prepare.py
\"\"\"
import sys
from pathlib import Path
import importlib.util

# æ·»åŠ æ¡†æ¶è·¯å¾„
sys.path.insert(0, '/path/to/data_science_agent_toolkit')

# å¯¼å…¥æ¡†æ¶çš„ prepare å‡½æ•°
prepare_file = Path('/path/to/data_science_agent_toolkit/mlebench/competitions/{comp_id}/prepare.py')
spec = importlib.util.spec_from_file_location("prepare_module", prepare_file)
prepare_module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(prepare_module)
prepare_fn = prepare_module.prepare

# å®šä¹‰è·¯å¾„
current_dir = Path(__file__).parent
raw_dir = current_dir / 'raw'
public_dir = current_dir / 'prepared' / 'public'
private_dir = current_dir / 'prepared' / 'private'

# åˆ›å»ºç›®å½•
public_dir.mkdir(parents=True, exist_ok=True)
private_dir.mkdir(parents=True, exist_ok=True)

if __name__ == '__main__':
    print("=" * 60)
    print("Preparing {comp_id}...")
    print("=" * 60)
    print(f"  Raw:     {{raw_dir}}")
    print(f"  Public:  {{public_dir}}")
    print(f"  Private: {{private_dir}}")
    print()

    # è°ƒç”¨æ¡†æ¶çš„ prepare å‡½æ•°
    prepare_fn(raw_dir, public_dir, private_dir)

    print()
    print("=" * 60)
    print("âœ“ Dataset prepared successfully!")
    print("=" * 60)

    # éªŒè¯ç”Ÿæˆçš„æ–‡ä»¶
    print("\\nGenerated files:")
    print("  Public:")
    for file in sorted(public_dir.glob("*")):
        size = file.stat().st_size / 1024  # KB
        print(f"    - {{file.name}} ({{size:.2f}} KB)")

    print("  Private:")
    for file in sorted(private_dir.glob("*")):
        size = file.stat().st_size / 1024  # KB
        print(f"    - {{file.name}} ({{size:.2f}} KB)")
"""


def convert_task(task_id: int, questions: Dict, labels: Dict, dry_run: bool = False, auto_prepare: bool = False):
    """è½¬æ¢å•ä¸ªä»»åŠ¡"""
    if task_id not in questions:
        print(f"âŒ Task {task_id} not found in questions")
        return False

    if task_id not in labels:
        print(f"âŒ Task {task_id} not found in labels")
        return False

    question_data = questions[task_id]
    label_data = labels[task_id]

    # ç”Ÿæˆæ¯”èµ› ID
    comp_id = create_competition_id(task_id, question_data['question'])
    print(f"\\n{'=' * 60}")
    print(f"Converting Task {task_id} -> {comp_id}")
    print(f"{'=' * 60}")

    # æ ¼å¼åŒ–ç­”æ¡ˆ
    answer_str = format_answer(label_data['common_answers'])
    print(f"Answer: {answer_str}")

    if dry_run:
        print("  [DRY RUN] Skipping file creation")
        return True

    # åˆ›å»ºæ¯”èµ›æ³¨å†Œç›®å½•
    comp_dir = MLEBENCH_COMPETITIONS_DIR / comp_id
    comp_dir.mkdir(parents=True, exist_ok=True)
    print(f"âœ“ Created competition directory: {comp_dir}")

    # åˆ›å»ºæ‰€æœ‰æ–‡ä»¶
    (comp_dir / 'config.yaml').write_text(create_config_yaml(comp_id, question_data['question'][:50]))
    (comp_dir / 'description.md').write_text(create_description_md(question_data))
    (comp_dir / 'grade.py').write_text(create_grade_py(label_data['common_answers']))
    (comp_dir / 'prepare.py').write_text(create_prepare_py(task_id, question_data['file_name'], answer_str))
    (comp_dir / 'leaderboard.csv').write_text(create_leaderboard_csv())
    (comp_dir / 'checksums.yaml').write_text(create_checksums_yaml(comp_id))
    print(f"âœ“ Created all competition files")

    # åˆ›å»ºæ•°æ®é›†ç›®å½•
    data_dir = DSFLOW_DATA_DIR / comp_id
    raw_dir = data_dir / 'raw'
    raw_dir.mkdir(parents=True, exist_ok=True)

    # å¤åˆ¶æ•°æ®æ–‡ä»¶
    source_file = DABENCH_DIR / 'da-dev-tables' / question_data['file_name']
    dest_file = raw_dir / question_data['file_name']
    if source_file.exists():
        import shutil
        shutil.copy(source_file, dest_file)
        print(f"âœ“ Copied data file: {dest_file}")
    else:
        print(f"âš  Warning: Data file not found: {source_file}")

    # åˆ›å»ºä¾¿æ·å‡†å¤‡è„šæœ¬
    (data_dir / 'prepare.py').write_text(create_dataset_prepare_py(comp_id))
    print(f"âœ“ Created dataset prepare script")

    # è‡ªåŠ¨å‡†å¤‡æ•°æ®ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if auto_prepare:
        print(f"\\nğŸ“¦ Auto-preparing data for {comp_id}...")
        try:
            import subprocess
            import sys
            result = subprocess.run(
                [sys.executable, str(data_dir / 'prepare.py')],
                cwd=str(data_dir),
                capture_output=True,
                text=True,
                timeout=60
            )
            if result.returncode == 0:
                print(f"âœ… Data prepared successfully!")
                # éªŒè¯æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                public_dir = data_dir / 'prepared' / 'public'
                private_dir = data_dir / 'prepared' / 'private'
                if (public_dir / 'train.csv').exists() and (private_dir / 'answer.csv').exists():
                    print(f"âœ“ Verified: All data files created")
                else:
                    print(f"âš  Warning: Some data files may be missing")
            else:
                print(f"âŒ Failed to prepare data:")
                print(result.stderr)
                return False
        except subprocess.TimeoutExpired:
            print(f"âŒ Data preparation timed out (>60s)")
            return False
        except Exception as e:
            print(f"âŒ Error during data preparation: {e}")
            return False

    print(f"âœ… Task {task_id} converted successfully!")
    return True


def main():
    parser = argparse.ArgumentParser(description='Convert DABench tasks to MLE-Bench format')
    parser.add_argument('--task-ids', type=int, nargs='+', help='Task IDs to convert')
    parser.add_argument('--all', action='store_true', help='Convert all tasks')
    parser.add_argument('--dry-run', action='store_true', help='Dry run (do not create files)')
    parser.add_argument('--auto-prepare', action='store_true', help='Automatically prepare data after conversion')
    parser.add_argument('--list', action='store_true', help='List all available tasks')

    args = parser.parse_args()

    # åŠ è½½æ•°æ®
    questions, labels = load_dabench_data()

    if args.list:
        print("\\nAvailable DABench tasks:")
        print(f"{'=' * 80}")
        for task_id in sorted(questions.keys()):
            q = questions[task_id]
            print(f"Task {task_id:3d} [{q['level']:6s}]: {q['question'][:60]}...")
        print(f"{'=' * 80}")
        print(f"Total: {len(questions)} tasks")
        return

    # ç¡®å®šè¦è½¬æ¢çš„ä»»åŠ¡
    if args.all:
        task_ids = sorted(questions.keys())
    elif args.task_ids:
        task_ids = args.task_ids
    else:
        print("Error: Please specify --task-ids or --all")
        parser.print_help()
        return

    # è½¬æ¢ä»»åŠ¡
    print(f"\\nConverting {len(task_ids)} task(s)...")
    if args.auto_prepare:
        print(f"âš¡ Auto-prepare mode enabled - data will be prepared automatically")
    success_count = 0
    for task_id in task_ids:
        if convert_task(task_id, questions, labels, dry_run=args.dry_run, auto_prepare=args.auto_prepare):
            success_count += 1

    print(f"\\n{'=' * 60}")
    print(f"Conversion complete: {success_count}/{len(task_ids)} tasks successful")
    print(f"{'=' * 60}")


if __name__ == '__main__':
    main()
