#!/usr/bin/env python3
"""
ä½¿ç”¨ Kaggle API è‡ªåŠ¨åˆ›å»º DSLighting é¡¹ç›®

è¿™ä¸ªå·¥å…·ä¼šï¼š
1. è°ƒç”¨ Kaggle API è·å–æ¯”èµ›ä¿¡æ¯
2. ä¸‹è½½æ‰€éœ€æ•°æ®æ–‡ä»¶
3. è‡ªåŠ¨æ£€æµ‹æ•°æ®æ ¼å¼
4. è½¬æ¢ä¸º DSLighting æ ‡å‡†æ ¼å¼
5. ç”Ÿæˆæ‰€æœ‰é…ç½®æ–‡ä»¶

ç”¨æ³•:
    python kaggle_auto_setup.py --competition titanic
    python kaggle_auto_setup.py --competition house-prices-advanced-regression-techniques
"""

import argparse
import json
import subprocess
import sys
import zipfile
from pathlib import Path
from typing import Dict, List, Tuple
import pandas as pd
import yaml
from kaggle.api.kaggle_api_extended import KaggleApi


class KaggleAPI:
    """Kaggle API å°è£…"""

    def __init__(self):
        """åˆå§‹åŒ– Kaggle API"""
        self.api = KaggleApi()
        try:
            self.api.authenticate()
        except Exception as e:
            print(f"âŒ Kaggle API è®¤è¯å¤±è´¥: {e}")
            print("è¯·ç¡®ä¿å·²é…ç½® KAGGLE_API_TOKEN ç¯å¢ƒå˜é‡æˆ– ~/.kaggle/kaggle.json æ–‡ä»¶")
            sys.exit(1)

    def get_competition_info(self, competition_id: str) -> Dict:
        """è·å–æ¯”èµ›ä¿¡æ¯"""
        print(f"ğŸ“¡ è·å–æ¯”èµ›ä¿¡æ¯: {competition_id}")

        try:
            # ä»æ¯”èµ›åˆ—è¡¨ä¸­è·å–
            competitions = self.api.competitions_list(search=competition_id)
            for comp in competitions:
                # åªæ£€æŸ¥ ref å±æ€§
                if hasattr(comp, 'ref') and comp.ref == competition_id:
                    info = {
                        "id": comp.ref,
                        "title": comp.title,
                        "description": comp.description if hasattr(comp, 'description') else "",
                        "reward": comp.rewardAmount if hasattr(comp, 'rewardAmount') else "",
                        "deadline": comp.deadline if hasattr(comp, 'deadline') else "",
                    }

                    return info

            # å¦‚æœæ²¡æœ‰ç²¾ç¡®åŒ¹é…ï¼Œå°è¯•æ¨¡ç³ŠåŒ¹é…
            for comp in competitions:
                if hasattr(comp, 'ref') and competition_id.lower() in comp.ref.lower():
                    info = {
                        "id": comp.ref,
                        "title": comp.title,
                        "description": comp.description if hasattr(comp, 'description') else "",
                        "reward": comp.rewardAmount if hasattr(comp, 'rewardAmount') else "",
                        "deadline": comp.deadline if hasattr(comp, 'deadline') else "",
                    }

                    return info

            return {"id": competition_id, "title": competition_id, "description": "", "full_description": ""}
        except Exception as e:
            print(f"âš ï¸  æ— æ³•è·å–æ¯”èµ›ä¿¡æ¯: {e}")
            return {"id": competition_id, "title": competition_id, "description": "", "full_description": ""}

    def list_files(self, competition_id: str) -> List[Dict]:
        """åˆ—å‡ºæ¯”èµ›æ–‡ä»¶"""
        print(f"ğŸ“‹ åˆ—å‡ºæ•°æ®æ–‡ä»¶: {competition_id}")

        try:
            files_response = self.api.competition_list_files(competition_id)
            # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
            files = []
            for f in files_response:
                files.append({
                    "name": f.name,
                    "size": f.totalBytes if hasattr(f, 'totalBytes') else 0,
                    "creationDate": str(f.creationDate) if hasattr(f, 'creationDate') else ""
                })
            return files
        except Exception as e:
            print(f"âš ï¸  æ— æ³•åˆ—å‡ºæ–‡ä»¶: {e}")
            return []

    def download_files(self, competition_id: str, dest_dir: Path):
        """ä¸‹è½½æ‰€æœ‰æ–‡ä»¶"""
        print(f"ğŸ“¥ ä¸‹è½½æ•°æ®æ–‡ä»¶: {competition_id}")

        dest_dir.mkdir(parents=True, exist_ok=True)

        try:
            self.api.competition_download_files(
                competition_id,
                path=str(dest_dir),
                quiet=False
            )
            print(f"âœ… ä¸‹è½½å®Œæˆ: {dest_dir}")
        except Exception as e:
            print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
            sys.exit(1)


class DataAnalyzer:
    """æ•°æ®åˆ†æå™¨"""

    def analyze_file(self, file_path: Path) -> Dict:
        """åˆ†ææ•°æ®æ–‡ä»¶"""
        try:
            if file_path.suffix == '.csv':
                return self._analyze_csv(file_path)
            elif file_path.suffix in ['.xlsx', '.xls']:
                return self._analyze_excel(file_path)
            elif file_path.suffix == '.parquet':
                return self._analyze_parquet(file_path)
            else:
                return {"type": "unknown", "error": "Unsupported format"}
        except Exception as e:
            return {"type": "error", "error": str(e)}

    def _analyze_csv(self, file_path: Path) -> Dict:
        """åˆ†æ CSV æ–‡ä»¶"""
        df = pd.read_csv(file_path, nrows=1000)

        # æ£€æµ‹æ˜¯å¦ä¸ºè®­ç»ƒé›†ï¼ˆåŒ…å«æ ‡ç­¾åˆ—ï¼‰
        label_cols = self._detect_label_columns(df)

        # æ£€æµ‹æ˜¯å¦ä¸ºæµ‹è¯•é›†
        is_test = 'test' in file_path.name.lower()

        # æ£€æµ‹æ˜¯å¦ä¸ºæäº¤ç¤ºä¾‹
        is_submission = any(word in file_path.name.lower()
                          for word in ['sample', 'submission'])

        return {
            "type": "csv",
            "shape": df.shape,
            "columns": list(df.columns),
            "dtypes": {col: str(dtype) for col, dtype in df.dtypes.items()},
            "label_columns": label_cols,
            "is_test": is_test,
            "is_submission": is_submission,
            "has_id": self._has_id_column(df),
        }

    def _detect_label_columns(self, df: pd.DataFrame) -> List[str]:
        """æ£€æµ‹å¯èƒ½çš„æ ‡ç­¾åˆ—"""
        label_cols = []

        # å¸¸è§çš„æ ‡ç­¾åˆ—å
        common_labels = ['target', 'label', 'class', 'price', 'survived',
                        'sales', 'demand', 'count', 'prediction']

        for col in df.columns:
            if col.lower() in common_labels:
                label_cols.append(col)

        # æ£€æŸ¥æ˜¯å¦ä¸ºåˆ†ç±»ä»»åŠ¡
        for col in df.columns:
            if df[col].nunique() < 20 and df[col].dtype in ['int64', 'object']:
                if col not in label_cols:
                    label_cols.append(col)

        return label_cols

    def _has_id_column(self, df: pd.DataFrame) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰ ID åˆ—"""
        id_keywords = ['id', 'ID', 'Id']
        return any(col in df.columns for col in id_keywords)

    def _analyze_excel(self, file_path: Path) -> Dict:
        """åˆ†æ Excel æ–‡ä»¶"""
        df = pd.read_excel(file_path, nrows=1000)
        return {
            "type": "excel",
            "shape": df.shape,
            "columns": list(df.columns),
        }

    def _analyze_parquet(self, file_path: Path) -> Dict:
        """åˆ†æ Parquet æ–‡ä»¶"""
        df = pd.read_parquet(file_path)
        return {
            "type": "parquet",
            "shape": df.shape,
            "columns": list(df.columns),
        }


class DSLightingSetup:
    """DSLighting é¡¹ç›®è®¾ç½®å™¨"""

    def __init__(self, competition_id: str, project_dir: Path = None):
        self.competition_id = competition_id
        self.project_dir = project_dir or Path.cwd()
        self.kaggle_api = KaggleAPI()
        self.analyzer = DataAnalyzer()

    def setup(self):
        """å®Œæ•´è®¾ç½®æµç¨‹"""
        print("="*80)
        print(f"ğŸš€ DSLighting + Kaggle è‡ªåŠ¨è®¾ç½®")
        print(f"æ¯”èµ›: {self.competition_id}")
        print("="*80)
        print()

        # 1. è·å–æ¯”èµ›ä¿¡æ¯
        comp_info = self.kaggle_api.get_competition_info(self.competition_id)
        competition_name = comp_info.get('title', self.competition_id)

        # 2. åˆ—å‡ºæ–‡ä»¶
        files = self.kaggle_api.list_files(self.competition_id)
        print(f"æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶")
        for file in files:
            print(f"  - {file.get('name', 'unknown')}")
        print()

        # 3. ä¸‹è½½æ–‡ä»¶
        raw_dir = self.project_dir / "data" / "raw" / self.competition_id
        self.kaggle_api.download_files(self.competition_id, raw_dir)

        # 4. è§£å‹æ–‡ä»¶
        extracted_files = self._extract_files(raw_dir)

        # 5. åˆ†ææ–‡ä»¶å¹¶åˆ†ç±»
        file_analysis = self._analyze_files(extracted_files)

        # 6. æ£€æµ‹è¯„ä¼°æŒ‡æ ‡
        metric = self._detect_metric(comp_info, file_analysis)

        # 7. å‡†å¤‡æ ‡å‡†æ ¼å¼
        self._prepare_standard_format(file_analysis, metric)

        # 8. ç”Ÿæˆé…ç½®æ–‡ä»¶
        self._generate_configs(competition_name, metric, file_analysis, comp_info)

        # 9. ç”Ÿæˆè¿è¡Œè„šæœ¬
        self._generate_scripts()

        # 10. åˆ›å»º README
        self._generate_readme(competition_name, comp_info)

        print()
        print("="*80)
        print("âœ… è®¾ç½®å®Œæˆï¼")
        print("="*80)
        print()
        self._print_next_steps()

    def _extract_files(self, raw_dir: Path) -> List[Path]:
        """è§£å‹ä¸‹è½½çš„æ–‡ä»¶"""
        print()
        print("ğŸ“¦ è§£å‹æ–‡ä»¶...")

        extracted_files = []

        # å¤„ç† zip æ–‡ä»¶
        for zip_file in raw_dir.glob("*.zip"):
            print(f"  è§£å‹: {zip_file.name}")
            with zipfile.ZipFile(zip_file, 'r') as zip_ref:
                zip_ref.extractall(raw_dir)
            zip_file.unlink()

        # åˆ—å‡ºæ‰€æœ‰æ•°æ®æ–‡ä»¶
        for ext in ['*.csv', '*.xlsx', '*.xls', '*.parquet']:
            extracted_files.extend(raw_dir.glob(ext))

        print(f"âœ… è§£å‹å®Œæˆï¼Œæ‰¾åˆ° {len(extracted_files)} ä¸ªæ•°æ®æ–‡ä»¶")
        return extracted_files

    def _analyze_files(self, files: List[Path]) -> Dict[str, List[Dict]]:
        """åˆ†ææ–‡ä»¶å¹¶åˆ†ç±»"""
        print()
        print("ğŸ” åˆ†ææ–‡ä»¶...")

        train_files = []
        test_files = []
        submission_files = []
        other_files = []

        for file_path in files:
            print(f"  åˆ†æ: {file_path.name}")
            analysis = self.analyzer.analyze_file(file_path)

            file_info = {
                "path": file_path,
                "analysis": analysis
            }

            if analysis.get('is_submission'):
                submission_files.append(file_info)
            elif analysis.get('is_test'):
                test_files.append(file_info)
            elif analysis.get('label_columns'):
                train_files.append(file_info)
            else:
                other_files.append(file_info)

        print()
        print(f"âœ… åˆ†æç»“æœ:")
        print(f"  è®­ç»ƒé›†: {len(train_files)} ä¸ª")
        print(f"  æµ‹è¯•é›†: {len(test_files)} ä¸ª")
        print(f"  æäº¤ç¤ºä¾‹: {len(submission_files)} ä¸ª")
        print(f"  å…¶ä»–æ–‡ä»¶: {len(other_files)} ä¸ª")

        return {
            "train": train_files,
            "test": test_files,
            "submission": submission_files,
            "other": other_files
        }

    def _detect_metric(self, comp_info: Dict, file_analysis: Dict) -> str:
        """æ£€æµ‹è¯„ä¼°æŒ‡æ ‡"""
        print()
        print("ğŸ“Š æ£€æµ‹è¯„ä¼°æŒ‡æ ‡...")

        # ä»æ¯”èµ›æè¿°æ£€æµ‹
        description = comp_info.get('description', '').lower()

        # å…³é”®è¯æ˜ å°„
        metric_keywords = {
            'accuracy': ['accuracy', 'classification', 'classify'],
            'rmse': ['rmse', 'root mean squared', 'regression'],
            'mae': ['mae', 'mean absolute', 'regression'],
            'rmsle': ['rmsle', 'root mean squared log'],
            'f1': ['f1', 'f-score'],
            'auc': ['auc', 'roc', 'area under curve'],
            'logloss': ['logloss', 'log loss'],
        }

        # æœç´¢å…³é”®è¯
        for metric, keywords in metric_keywords.items():
            if any(keyword in description for keyword in keywords):
                print(f"âœ… æ£€æµ‹åˆ°æŒ‡æ ‡: {metric}")
                return metric

        # é»˜è®¤ä½¿ç”¨ accuracy
        print("âš ï¸  æ— æ³•è‡ªåŠ¨æ£€æµ‹ï¼Œä½¿ç”¨é»˜è®¤æŒ‡æ ‡: accuracy")
        return "accuracy"

    def _prepare_standard_format(self, file_analysis: Dict, metric: str):
        """å‡†å¤‡ DSLighting æ ‡å‡†æ ¼å¼"""
        print()
        print("ğŸ“ å‡†å¤‡ DSLighting æ ‡å‡†æ ¼å¼...")

        prepared_public = self.project_dir / "data" / "competitions" / self.competition_id / "prepared" / "public"
        prepared_private = self.project_dir / "data" / "competitions" / self.competition_id / "prepared" / "private"

        prepared_public.mkdir(parents=True, exist_ok=True)
        prepared_private.mkdir(parents=True, exist_ok=True)

        # 1. å¤„ç†è®­ç»ƒé›†ï¼šåˆ†å‰²æˆ train + validation (for test_answer)
        if file_analysis["train"]:
            train_file = file_analysis["train"][0]["path"]
            print(f"  ğŸ“Š åˆ†å‰²è®­ç»ƒé›†...")

            # è¯»å–è®­ç»ƒæ•°æ®
            train_df = pd.read_csv(train_file)

            # åˆ†å‰²ï¼š80% è®­ç»ƒï¼Œ20% éªŒè¯
            from sklearn.model_selection import train_test_split
            train_split, val_split = train_test_split(
                train_df,
                test_size=0.2,
                random_state=42
            )

            # ä¿å­˜æ–°çš„è®­ç»ƒé›†
            new_train_path = prepared_public / "train.csv"
            train_split.to_csv(new_train_path, index=False)
            print(f"  âœ… è®­ç»ƒé›†: train.csv ({len(train_split)} è¡Œ)")

            # 2. ä»éªŒè¯é›†åˆ›å»º test_answer.csv
            if file_analysis["submission"]:
                submission_file = file_analysis["submission"][0]["path"]
                sample_sub = pd.read_csv(submission_file)

                # è·å– ID åˆ—åï¼ˆé€šå¸¸æ˜¯ç¬¬ä¸€åˆ—ï¼‰
                id_col = sample_sub.columns[0]

                # è·å–ç›®æ ‡åˆ—åï¼ˆé€šå¸¸æ˜¯æœ€åä¸€åˆ—ï¼Œé™¤äº† ID åˆ—ï¼‰
                target_col = sample_sub.columns[-1]

                # ä»éªŒè¯é›†åˆ›å»º test_answer
                val_ids = val_split[id_col].values
                val_labels = val_split[target_col].values

                # æŒ‰ç…§ sampleSubmission çš„æ ¼å¼åˆ›å»º test_answer
                test_answer = pd.DataFrame({
                    id_col: val_ids,
                    target_col: val_labels
                })

                test_answer_path = prepared_private / "test_answer.csv"
                test_answer.to_csv(test_answer_path, index=False)
                print(f"  âœ… ç­”æ¡ˆæ–‡ä»¶: test_answer.csv ({len(test_answer)} è¡Œï¼Œä»è®­ç»ƒé›†åˆ†å‰²)")
        else:
            # æ²¡æœ‰è®­ç»ƒé›†ï¼Œåˆ›å»ºå ä½ç¬¦
            print(f"  âš ï¸  æœªæ‰¾åˆ°è®­ç»ƒé›†ï¼Œåˆ›å»ºå ä½ç¬¦")
            answer_placeholder = prepared_private / "test_answer.csv"
            if file_analysis["submission"]:
                sample_df = pd.read_csv(file_analysis["submission"][0]["path"])
                sample_df.iloc[:, 1:] = 0  # å ä½ç¬¦å€¼
                sample_df.to_csv(answer_placeholder, index=False)
                print(f"  âš ï¸  åˆ›å»º test_answer å ä½ç¬¦")

        # 3. å¤åˆ¶æµ‹è¯•é›†ï¼ˆKaggle åŸå§‹æµ‹è¯•é›†ï¼Œç”¨äºæœ€ç»ˆé¢„æµ‹ï¼‰
        if file_analysis["test"]:
            test_file = file_analysis["test"][0]["path"]
            dest = prepared_public / "test.csv"
            self._convert_to_csv(test_file, dest)
            print(f"  âœ… æµ‹è¯•é›†: test.csv (Kaggle åŸå§‹æµ‹è¯•é›†ï¼Œæ— æ ‡ç­¾)")

        # 4. å¤åˆ¶æäº¤ç¤ºä¾‹
        if file_analysis["submission"]:
            submission_file = file_analysis["submission"][0]["path"]
            dest = prepared_public / "sampleSubmission.csv"
            self._convert_to_csv(submission_file, dest)
            print(f"  âœ… æäº¤ç¤ºä¾‹: sampleSubmission.csv")

        print(f"\nâœ… æ•°æ®å‡†å¤‡å®Œæˆ: {prepared_public}")
        print(f"  ğŸ“ åŸå§‹æ–‡ä»¶ä¿å­˜åœ¨: {file_analysis['train'][0]['path'].parent.parent if file_analysis['train'] else 'N/A'}")

    def _convert_to_csv(self, src: Path, dest: Path):
        """è½¬æ¢ä¸º CSV æ ¼å¼"""
        if src.suffix == '.csv':
            # ç›´æ¥å¤åˆ¶
            import shutil
            shutil.copy(src, dest)
        elif src.suffix in ['.xlsx', '.xls']:
            # Excel è½¬ CSV
            pd.read_excel(src).to_csv(dest, index=False)
        elif src.suffix == '.parquet':
            # Parquet è½¬ CSV
            pd.read_parquet(src).to_csv(dest, index=False)

    def _generate_configs(self, competition_name: str, metric: str, file_analysis: Dict, comp_info: Dict):
        """ç”Ÿæˆé…ç½®æ–‡ä»¶"""
        print()
        print("âš™ï¸  ç”Ÿæˆé…ç½®æ–‡ä»¶...")

        registry_dir = self.project_dir / "registry" / self.competition_id
        registry_dir.mkdir(parents=True, exist_ok=True)

        # config.yaml
        config = {
            'id': self.competition_id,
            'name': competition_name,
            'competition_type': 'simple',
            'task_type': 'kaggle',
            'awards_medals': False,
            'description': 'description.md',
            'dataset': {
                'answers': f'{self.competition_id}/prepared/private/test_answer.csv',
                'sample_submission': f'{self.competition_id}/prepared/public/sampleSubmission.csv',
            },
            'grader': {
                'name': metric,
                'grade_fn': 'grade:grade',
            }
        }

        config_path = registry_dir / "config.yaml"
        with open(config_path, 'w') as f:
            yaml.dump(config, f, default_flow_style=False, sort_keys=False)
        print(f"  âœ… config.yaml")

        # grade.py
        grade_code = f'''"""
{competition_name} - è¯„ä¼°å™¨
"""

import pandas as pd
import numpy as np
from pathlib import Path


def grade(submission_path: str, answer_path: str) -> dict:
    """
    è¯„ä¼°æäº¤ç»“æœ

    Args:
        submission_path: æäº¤æ–‡ä»¶è·¯å¾„
        answer_path: ç­”æ¡ˆæ–‡ä»¶è·¯å¾„

    Returns:
        è¯„ä¼°ç»“æœå­—å…¸
    """
    # è¯»å–æ–‡ä»¶
    submission = pd.read_csv(submission_path)
    answers = pd.read_csv(answer_path)

    # åˆå¹¶æ•°æ®
    # æ ¹æ®ä½ çš„æ•°æ®è°ƒæ•´åˆ—å
    id_col = submission.columns[0]  # å‡è®¾ç¬¬ä¸€åˆ—æ˜¯ ID
    merged = submission.merge(answers, on=id_col, suffixes=('_pred', '_true'))

    # è®¡ç®—æŒ‡æ ‡
    metric = "{metric}"

    if metric == "accuracy":
        from sklearn.metrics import accuracy_score
        pred_col = submission.columns[1]
        true_col = answers.columns[1]
        score = accuracy_score(merged[true_col], merged[pred_col])
    elif metric == "rmse":
        from sklearn.metrics import mean_squared_error
        pred_col = submission.columns[1]
        true_col = answers.columns[1]
        score = np.sqrt(mean_squared_error(merged[true_col], merged[pred_col]))
    elif metric == "mae":
        from sklearn.metrics import mean_absolute_error
        pred_col = submission.columns[1]
        true_col = answers.columns[1]
        score = mean_absolute_error(merged[true_col], merged[pred_col])
    else:
        raise ValueError(f"Unknown metric: {{metric}}")

    return {{
        'score': score,
        '{metric}': score,
        'num_samples': len(merged),
        'valid_submission': True
    }}


if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1:
        submission_file = sys.argv[1]
    else:
        submission_file = "data/competitions/{self.competition_id}/prepared/public/sampleSubmission.csv"

    if len(sys.argv) > 2:
        answer_file = sys.argv[2]
    else:
        answer_file = "data/competitions/{self.competition_id}/prepared/private/test_answer.csv"

    result = grade(submission_file, answer_file)
    print(f"å¾—åˆ†: {{result['score']:.4f}}")
'''

        grade_path = registry_dir / "grade.py"
        with open(grade_path, 'w') as f:
            f.write(grade_code)
        print(f"  âœ… grade.py")

        # description.md
        # ä½¿ç”¨ä» Kaggle API è·å–çš„çœŸå®ä¿¡æ¯
        comp_desc = comp_info.get('description', '').strip()

        description = f'''# {competition_name}

{comp_desc if comp_desc else 'Predict outcomes for this competition.'}
'''

        desc_path = registry_dir / "description.md"
        with open(desc_path, 'w') as f:
            f.write(description)
        print(f"  âœ… description.md")

    def _generate_scripts(self):
        """ç”Ÿæˆè¿è¡Œè„šæœ¬"""
        print()
        print("ğŸ”§ ç”Ÿæˆè¿è¡Œè„šæœ¬...")

        # run.py
        run_script = f'''#!/usr/bin/env python3
"""
è¿è¡Œ DSLighting Agent
"""

import sys
from pathlib import Path

# åŠ è½½ç¯å¢ƒå˜é‡
from dotenv import load_dotenv
load_dotenv()

try:
    import dslighting
except ImportError:
    print("âŒ æœªæ‰¾åˆ° dslightingï¼Œè¯·å…ˆå®‰è£…:")
    print("   pip install dslighting")
    sys.exit(1)


def main():
    print("="*80)
    print("è¿è¡Œ DSLighting Agent")
    print("="*80)
    print()

    try:
        # åŠ è½½æ•°æ®ï¼ˆæ˜¾å¼æŒ‡å®šæ•°æ®è·¯å¾„å’Œæ³¨å†Œè¡¨è·¯å¾„ï¼‰
        data = dslighting.load_data(
            "data/competitions/{self.competition_id}",
            registry_dir="registry/{self.competition_id}"
        )

        # æ˜¾ç¤ºæ•°æ®ä¿¡æ¯
        print(data.show())
        print()

        # è¿è¡Œ Agent
        print("æ­£åœ¨è¿è¡Œ Agent...")
        agent = dslighting.Agent(
            model="openai/gpt-4",  # æˆ– "gpt-3.5-turbo"
            workflow="aide",
            max_iterations=10,
        )
        result = agent.run(data)

        print()
        print("="*80)
        print("ğŸ¯ ç»“æœ")
        print("="*80)
        print(f"åˆ†æ•°: {{result.score}}")
        print(f"æäº¤æ–‡ä»¶: {{result.output}}")
        if result.workspace_path:
            print(f"å·¥ä½œç©ºé—´: {{result.workspace_path}}")
        if result.artifacts_path:
            print(f"äº§ç‰©ç›®å½•: {{result.artifacts_path}}")

    except Exception as e:
        print(f"âŒ é”™è¯¯: {{e}}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
'''

        run_path = self.project_dir / "run.py"
        with open(run_path, 'w') as f:
            f.write(run_script)
        print(f"  âœ… run.py")

    def _generate_readme(self, competition_name: str, comp_info: Dict):
        """ç”Ÿæˆ README"""
        readme_path = self.project_dir / "README.md"

        content = f'''# {competition_name}

ä½¿ç”¨ DSLighting å‚åŠ  Kaggle [{competition_name}] æ¯”èµ›ã€‚

## ğŸ“Š æ¯”èµ›ä¿¡æ¯

- **æ¯”èµ› ID**: {self.competition_id}
- **æ¯”èµ›åç§°**: {competition_name}
- **å¥–åŠ±**: {comp_info.get('reward', 'N/A')}
- **å›¢é˜Ÿæ•°**: {comp_info.get('teamCount', 'N/A')}
- **å‚èµ›è€…æ•°**: {comp_info.get('userRanksTotal', 'N/A')}

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
pip install dslighting kaggle
```

### 2. é…ç½® Kaggle API

```bash
mkdir -p ~/.kaggle
# ä¸‹è½½ kaggle.json åç§»åŠ¨åˆ° ~/.kaggle/
chmod 600 ~/.kaggle/kaggle.json
```

### 3. æ•°æ®å·²å‡†å¤‡å¥½

æ•°æ®å·²ç»é€šè¿‡ Kaggle API è‡ªåŠ¨ä¸‹è½½å¹¶è½¬æ¢ä¸º DSLighting æ ¼å¼ã€‚

### 4. é…ç½®ç¯å¢ƒå˜é‡

å¤åˆ¶ `.env.example` ä¸º `.env` å¹¶å¡«å†™ä½ çš„ API é…ç½®ï¼š

```bash
cp .env.example .env
# ç¼–è¾‘ .env å¡«å†™ API_KEYã€API_BASE ç­‰é…ç½®
```

### 5. è¿è¡Œ DSLighting

```bash
python run.py
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/{self.competition_id}/      # åŸå§‹ä¸‹è½½çš„æ•°æ®
â”‚   â””â”€â”€ competitions/{self.competition_id}/  # DSLighting æ ¼å¼
â”‚       â””â”€â”€ prepared/
â”‚           â”œâ”€â”€ public/                 # è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
â”‚           â””â”€â”€ private/                # ç­”æ¡ˆæ•°æ®
â”œâ”€â”€ registry/{self.competition_id}/     # Registry é…ç½®
â”‚   â”œâ”€â”€ config.yaml
â”‚   â”œâ”€â”€ grade.py
â”‚   â””â”€â”€ description.md
â”œâ”€â”€ run.py                              # è¿è¡Œè„šæœ¬
â”œâ”€â”€ .env.example                        # ç¯å¢ƒå˜é‡æ¨¡æ¿
â”œâ”€â”€ .env                                # ç¯å¢ƒå˜é‡ï¼ˆéœ€è¦åˆ›å»ºï¼‰
â””â”€â”€ README.md                           # æœ¬æ–‡ä»¶
```

## âš ï¸ é‡è¦æç¤º

**test_answer.csv**: è¿™ä¸ªæ–‡ä»¶æ˜¯å ä½ç¬¦ï¼Œç”¨äºæœ¬åœ°éªŒè¯ã€‚ä½ éœ€è¦ï¼š

1. ä» Kaggle Discussion è·å–åŸºå‡†ç­”æ¡ˆ
2. æˆ–ä½¿ç”¨äº¤å‰éªŒè¯
3. æˆ–ä»è®­ç»ƒé›†åˆ†å‰²å‡ºéªŒè¯é›†

## ğŸ“¤ æäº¤åˆ° Kaggle

```bash
kaggle competitions submit -c {self.competition_id} \\
  -f <submission_file> \\
  -m "Generated by DSLighting"
```

## ğŸ”— ç›¸å…³é“¾æ¥

- [Kaggle æ¯”èµ›é¡µé¢](https://www.kaggle.com/c/{self.competition_id})
- [DSLighting æ–‡æ¡£](https://github.com/usail-hkust/dslighting)

---

ä½¿ç”¨ DSLighting + Kaggle API è‡ªåŠ¨ç”Ÿæˆ ğŸš€
'''

        with open(readme_path, 'w') as f:
            f.write(content)
        print(f"  âœ… README.md")

        # .env.example
        env_example = self.project_dir / ".env.example"
        with open(env_example, 'w') as f:
            f.write("""# API é…ç½®
API_KEY=your-api-key-here
API_BASE=https://api.openai.com/v1
LLM_MODEL=openai/gpt-4
""")
        print(f"  âœ… .env.example")
        print(f"  âš ï¸  è¯·å¤åˆ¶ .env.example ä¸º .env å¹¶å¡«å†™ä½ çš„ API é…ç½®")

    def _print_next_steps(self):
        """æ‰“å°ä¸‹ä¸€æ­¥æ“ä½œ"""
        print("ğŸ“‹ ä¸‹ä¸€æ­¥:")
        print()
        print("1. å¤åˆ¶ .env.example ä¸º .env å¹¶å¡«å†™ API é…ç½®")
        print("2. æŸ¥çœ‹ README.md äº†è§£æ¯”èµ›ä¿¡æ¯")
        print("3. è¿è¡Œ: python run.py")
        print("4. æäº¤ç»“æœåˆ° Kaggle")
        print()
        print("ğŸ’¡ æç¤º: ä½ å¯ä»¥ä¿®æ”¹ run.py ä¸­çš„å‚æ•°æ¥è°ƒæ•´ Agent è¡Œä¸º")


def main():
    parser = argparse.ArgumentParser(
        description="ä½¿ç”¨ Kaggle API è‡ªåŠ¨åˆ›å»º DSLighting é¡¹ç›®",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python kaggle_auto_setup.py --competition titanic
  python kaggle_auto_setup.py --competition house-prices-advanced-regression-techniques
  python kaggle_auto_setup.py --competition digit-recognizer --dir ./my-project
        """
    )

    parser.add_argument(
        "--competition", "-c",
        required=True,
        help="Kaggle competition ID (slug from URL)"
    )
    parser.add_argument(
        "--dir", "-d",
        default=None,
        help="é¡¹ç›®ç›®å½•ï¼ˆé»˜è®¤ä¸ºå½“å‰ç›®å½•ï¼‰"
    )

    args = parser.parse_args()

    # æ£€æŸ¥ Kaggle API é…ç½®ï¼ˆæ”¯æŒ kaggle.json æˆ–ç¯å¢ƒå˜é‡ï¼‰
    import os
    kaggle_json = Path.home() / ".kaggle" / "kaggle.json"
    has_api_token = os.environ.get("KAGGLE_API_TOKEN") is not None

    if not kaggle_json.exists() and not has_api_token:
        print("âŒ æœªæ‰¾åˆ° Kaggle API é…ç½®")
        print()
        print("è¯·é€‰æ‹©ä»¥ä¸‹ä»»ä¸€æ–¹å¼é…ç½®:")
        print()
        print("æ–¹å¼1: ä½¿ç”¨ç¯å¢ƒå˜é‡ï¼ˆæ¨èï¼‰")
        print("1. è®¿é—®: https://www.kaggle.com/")
        print("2. ç™»å½• â†’ è´¦æˆ·è®¾ç½® â†’ API â†’ Create New API Token")
        print("3. å¤åˆ¶ API Token")
        print("4. è¿è¡Œ: export KAGGLE_API_TOKEN=ä½ çš„Token")
        print()
        print("æ–¹å¼2: ä½¿ç”¨é…ç½®æ–‡ä»¶")
        print("1. è®¿é—®: https://www.kaggle.com/")
        print("2. ç™»å½• â†’ è´¦æˆ·è®¾ç½® â†’ API â†’ Create New API Token")
        print("3. ä¸‹è½½ kaggle.json")
        print("4. è¿è¡Œ: mkdir -p ~/.kaggle")
        print("5. è¿è¡Œ: mv ~/Downloads/kaggle.json ~/.kaggle/")
        print("6. è¿è¡Œ: chmod 600 ~/.kaggle/kaggle.json")
        sys.exit(1)

    if has_api_token:
        print("âœ… ä½¿ç”¨ç¯å¢ƒå˜é‡ KAGGLE_API_TOKEN")
    else:
        print("âœ… ä½¿ç”¨é…ç½®æ–‡ä»¶ ~/.kaggle/kaggle.json")

    # åˆ›å»ºé¡¹ç›®ç›®å½•
    project_dir = Path(args.dir) if args.dir else Path.cwd()

    # è¿è¡Œè®¾ç½®
    setup = DSLightingSetup(args.competition, project_dir)
    setup.setup()


if __name__ == "__main__":
    main()
