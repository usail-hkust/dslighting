# Custom Benchmark Example

This example shows how to add a custom data science benchmark to the DSAT framework using a full DABench-style structure.

## ðŸ“‹ Contents

- [Overview](#overview)
- [Example Task](#example-task)
- [Directory Layout](#directory-layout)
- [Core Components](#core-components)
- [Quickstart](#quickstart)
- [Extension Guide](#extension-guide)
- [Framework Integration](#framework-integration)

---

## Overview

### What is a DSAT Benchmark?

A benchmark is the DSAT component used to evaluate how an AI agent performs on a specific task. A complete benchmark includes:
- **Task definition**: what problem to solve
- **Datasets**: training and test data
- **Scoring function**: how to measure the agent
- **Workflow interface**: how the agent interacts with the task

### Why build a custom benchmark?

- âœ… Measure an agent on your own domain
- âœ… Standardize test flow and scoring rules
- âœ… Reproduce experiments
- âœ… Compare different agents fairly

---

## Example Task

This example implements a **house price prediction benchmark**:

- **Task type**: Regression
- **Input**: House features (area, bedrooms, age, location score)
- **Output**: Predicted house price (USD)
- **Metric**: RMSE (Root Mean Squared Error)

It is a typical data science task that demonstrates the full prepare â†’ train/test â†’ score flow.

---

## Directory Layout

```
examples/custom_benchmark/en/
â”œâ”€â”€ README.md                                    # This doc
â”œâ”€â”€ QUICKSTART.md                                # Quickstart guide
â”œâ”€â”€ prepare_example_data.py                      # Data generator
â”œâ”€â”€ custom_benchmark.py                          # Benchmark class
â”œâ”€â”€ run_example.sh                               # One-click script
â”‚
â”œâ”€â”€ competitions/                                # Competition registry
â”‚   â””â”€â”€ custom_house_price_prediction/
â”‚       â”œâ”€â”€ config.yaml                          # Competition config
â”‚       â”œâ”€â”€ description.md                       # Task description
â”‚       â”œâ”€â”€ grade.py                             # Scoring logic
â”‚       â”œâ”€â”€ prepare.py                           # Data preparation
â”‚       â”œâ”€â”€ leaderboard.csv                      # Sample leaderboard
â”‚       â””â”€â”€ checksums.yaml                       # Data checksum
â”‚
â””â”€â”€ data/                                        # Dataset
    â””â”€â”€ custom-house-price-prediction/
        â”œâ”€â”€ raw/                                 # Raw data
        â”‚   â””â”€â”€ houses.csv
        â””â”€â”€ prepared/                            # Prepared splits
            â”œâ”€â”€ public/                          # Visible to agent
            â”‚   â”œâ”€â”€ train.csv                    # Train data
            â”‚   â””â”€â”€ sample_submission.csv        # Submission format
            â””â”€â”€ private/                         # Hidden answers
                â””â”€â”€ answer.csv                   # Test labels
```

---

## Core Components

### 1. Competition registry (`competitions/`)

DABench-style metadata and data pipelines:

#### `config.yaml` - competition config
```yaml
id: custom-house-price-prediction
name: House Price Prediction Challenge
competition_type: kaggle
grader:
  name: rmse
  grade_fn: competitions.custom_house_price_prediction.grade:grade
preparer: competitions.custom_house_price_prediction.prepare:prepare
```

#### `description.md` - task details
- Task goal and context
- Feature descriptions
- Submission format
- Scoring rules

#### `prepare.py` - data preparation
```python
def prepare(raw: Path, public: Path, private: Path):
    """
    Split raw data into:
    - public/: train data (80%)
    - private/: test answers (20%)
    """
```

#### `grade.py` - scoring function
```python
def grade(submission: pd.DataFrame, answers: pd.DataFrame) -> float:
    """Compute RMSE"""
    rmse = np.sqrt(np.mean((predicted - actual) ** 2))
    return rmse
```

### 2. Dataset directory (`data/`)

Organized similarly to DSFlow:

```
data/custom-house-price-prediction/
â”œâ”€â”€ raw/                       # Generated by prepare_example_data.py
â”‚   â””â”€â”€ houses.csv
â””â”€â”€ prepared/                  # Produced by prepare.py
    â”œâ”€â”€ public/                # Visible to the agent
    â”‚   â”œâ”€â”€ train.csv          # Features + label
    â”‚   â””â”€â”€ sample_submission.csv  # Submission template
    â””â”€â”€ private/               # Hidden (used for scoring)
        â””â”€â”€ answer.csv         # Ground truth answers
```

### 3. Benchmark class (`custom_benchmark.py`)

Implementation derived from `BaseBenchmark`:

```python
class HousePriceBenchmark(BaseBenchmark):
    def _load_problems(self) -> List[Dict]:
        """Load the task list"""

    def get_result_columns(self) -> List[str]:
        """Define result CSV columns"""

    async def evaluate_problem(self, problem, eval_fn) -> Tuple:
        """Run and score a single task"""
```

---

## Quickstart

### Step 1: Generate raw data

```bash
cd examples/custom_benchmark/en
python prepare_example_data.py
```

**Output**: `data/custom-house-price-prediction/raw/houses.csv` (100 rows)

### Step 2: Prepare train/test splits

```bash
cd competitions/custom_house_price_prediction
python prepare.py
```

**Output**:
- `data/.../prepared/public/train.csv` (80 rows)
- `data/.../prepared/public/sample_submission.csv`
- `data/.../prepared/private/answer.csv` (20 rows)

### Step 3: Test the benchmark

```bash
cd examples/custom_benchmark/en
python custom_benchmark.py
```

**Output**: Runs the mock evaluator, produces random predictions, and computes RMSE.

### One-click run (recommended)

```bash
bash run_example.sh
```

See [QUICKSTART.md](QUICKSTART.md) for details.

---

## Extension Guide

### Add a new task

1. **Create the competition directory**:
   ```bash
   mkdir -p competitions/my-new-task
   mkdir -p data/my-new-task/raw
   ```

2. **Create the core files**:
   - `config.yaml` - task metadata
   - `description.md` - task requirements
   - `prepare.py` - data preparation logic
   - `grade.py` - scoring logic

3. **Update the benchmark class**:
   ```python
   def _load_problems(self):
       return [
           {"task_id": "my-new-task", ...},
           # add more tasks here
       ]
   ```

### Customize scoring

Modify `competitions/*/grade.py` for different metrics:

```python
# Classification - Accuracy
def grade(submission, answers):
    accuracy = (submission['predicted'] == answers['actual']).mean()
    return accuracy

# Ranking - NDCG
def grade(submission, answers):
    from sklearn.metrics import ndcg_score
    score = ndcg_score(answers, submission)
    return score
```

### Support other task types

Set the task type inside `TaskDefinition`:

```python
# Kaggle-style (file I/O)
task = TaskDefinition(
    task_id="task-001",
    task_type="kaggle",
    payload={
        "public_data_dir": "./data/public",
        "output_submission_path": "./output.csv"
    }
)

# QA-style (text Q&A)
task = TaskDefinition(
    task_id="qa-001",
    task_type="qa",
    payload={
        "question": "What is the capital of France?"
    }
)
```

---

## Framework Integration

### Option 1: Register in `run_benchmark.py`

Edit `run_benchmark.py`:

```python
# Import the custom benchmark
from examples.custom_benchmark.custom_benchmark import HousePriceBenchmark

# Register in BENCHMARK_CLASSES
BENCHMARK_CLASSES = {
    "mle": MLEBenchmark,
    "dabench": MLEBenchmark,
    "house_price": HousePriceBenchmark,  # add this line
}
```

### Option 2: Add CLI args (optional)

If you need custom parameters:

```python
parser.add_argument(
    "--custom-data-dir",
    type=str,
    default=None,
    help="Path to custom benchmark data"
)

# Use it during initialization
if args.benchmark == "house_price":
    benchmark_kwargs["data_dir"] = args.custom_data_dir
```

### Run the custom benchmark

```bash
python run_benchmark.py \
  --workflow aide \
  --benchmark house_price \
  --log-path ./runs/house_price_results
```

---

## Reference Implementations

| Benchmark | File | Task Type | Notes |
|-----------|------|-----------|-------|
| **MLEBenchmark** | `dsat/benchmark/mle.py` | Kaggle-style | Production-ready, multi-competition |
| **DataSciBenchmark** | `dsat/benchmark/datasci.py` | Multi-step workflow | Complex data science flows |
| **HousePriceBenchmark** | `custom_benchmark.py` | Regression | This example, full DABench-style |

---

## FAQ

### Q: How do I verify the prepared data?

```bash
# Check generated files
ls -lh data/custom-house-price-prediction/prepared/public/
ls -lh data/custom-house-price-prediction/prepared/private/

# Inspect stats
python -c "
import pandas as pd
train = pd.read_csv('data/custom-house-price-prediction/prepared/public/train.csv')
print(f'Train rows: {len(train)}')
print(train.describe())
"
```

### Q: How do I debug the grader?

```bash
cd competitions/custom_house_price_prediction
python grade.py  # runs the built-in tests
```

### Q: How do I change dataset size?

Edit `prepare_example_data.py`:
```python
df = generate_house_data(n_samples=500)  # change from 100 to 500
```

---

## Next Steps

- ðŸ“– Read [QUICKSTART.md](QUICKSTART.md) for the walkthrough
- ðŸ”§ Browse `competitions/*/` to see implementations
- ðŸš€ Run `bash run_example.sh` for the full flow
- ðŸ“š Check `dsat/benchmark/mle.py` for a production-grade reference

---

**Author**: DS-Lighting Team  
**Version**: 1.0  
**Updated**: 2025-12
